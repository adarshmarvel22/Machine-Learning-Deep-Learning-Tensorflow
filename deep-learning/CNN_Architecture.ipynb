{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ans1.\n",
        "**Pooling in CNN:**\n",
        "   Pooling is a fundamental operation in Convolutional Neural Networks (CNNs) used primarily for dimensionality reduction and feature extraction. Its main purpose is to downsample the spatial dimensions of feature maps, which helps reduce the computational complexity of the network and controls overfitting. The benefits of pooling include:\n",
        "\n",
        "   - **Spatial Hierarchical Representation:** Pooling preserves the most important features while discarding less relevant details, creating a hierarchical representation of features. This helps in capturing increasingly abstract and higher-level information in deeper layers of the network.\n",
        "\n",
        "   - **Translation Invariance:** Pooling makes the network less sensitive to small spatial translations of features, which can be useful for tasks like object recognition, where the position of an object in an image may vary.\n",
        "\n",
        "   - **Reduced Computational Load:** Smaller feature maps after pooling require fewer parameters and computations in subsequent layers, making the network more computationally efficient.\n",
        "\n",
        "   - **Regularization:** Pooling can act as a form of regularization by preventing overfitting, as it reduces the spatial resolution of feature maps and encourages the network to focus on more dominant features.\n",
        "\n",
        "# ans 2.\n",
        " **Min Pooling vs. Max Pooling:**\n",
        "   Both min pooling and max pooling are pooling techniques used in CNNs, but they differ in how they select the values to be propagated to the next layer:\n",
        "\n",
        "   - **Max Pooling:** In max pooling, for each pooling region (usually a small square or rectangular area), the maximum value in that region is retained while all other values are discarded. Max pooling is known for preserving the most dominant features in a region and is commonly used for tasks like object recognition.\n",
        "\n",
        "   - **Min Pooling:** In min pooling, the minimum value in each pooling region is retained while discarding all other values. Min pooling is less common than max pooling and is used when you want to focus on the least prominent features.\n",
        "\n",
        "# ans 3.\n",
        " **Padding in CNN:**\n",
        "   Padding is the process of adding extra border pixels around the input image or feature map before applying convolutional operations. It is significant for several reasons:\n",
        "\n",
        "   - **Preservation of Spatial Dimensions:** Padding allows the output feature maps to have the same spatial dimensions as the input. Without padding, the spatial dimensions would decrease after each convolutional layer, potentially losing important spatial information.\n",
        "\n",
        "   - **Centering Convolution:** Padding ensures that the convolutional filter's center aligns with the pixels in the input, which is important for learning spatial hierarchies and maintaining symmetry in the network.\n",
        "\n",
        "   - **Avoiding Information Loss:** Padding helps prevent the loss of information at the borders of the input image, ensuring that the convolutional operation considers all parts of the input.\n",
        "\n",
        "# ans 4.\n",
        " **Zero-Padding vs. Valid-Padding:**\n",
        "   Zero-padding and valid-padding are two common types of padding used in CNNs, and they have different effects on the output feature map size:\n",
        "\n",
        "   - **Zero-Padding:** In zero-padding, extra rows and columns filled with zeros are added around the input image or feature map. This ensures that the output feature map has the same spatial dimensions as the input. Zero-padding is commonly used when you want to preserve spatial information, especially at the edges of the image.\n",
        "\n",
        "   - **Valid-Padding:** In valid-padding (also known as no-padding), no extra rows or columns are added around the input. As a result, the spatial dimensions of the output feature map are reduced compared to the input. Valid-padding is used when you want to reduce the spatial dimensions and focus on extracting essential features."
      ],
      "metadata": {
        "id": "wQG77HSEWBEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring LeNet"
      ],
      "metadata": {
        "id": "RB_YhV7qWN2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ans 1.\n",
        " **Overview of LeNet-5:**\n",
        "   LeNet-5 is a classic convolutional neural network (CNN) architecture developed by Yann LeCun in the early 1990s. It was designed primarily for handwritten digit recognition, making it one of the pioneering architectures in the field of deep learning and computer vision. LeNet-5 played a significant role in popularizing the use of convolutional layers for feature extraction in image data.\n",
        "\n",
        "# ans 2.\n",
        " **Key Components of LeNet-5:**\n",
        "   LeNet-5 consists of several key components, each serving a specific purpose in the network:\n",
        "\n",
        "   - **Input Layer:** LeNet-5 takes grayscale images of fixed size (usually 32x32 pixels) as input.\n",
        "\n",
        "   - **Convolutional Layers:** LeNet-5 includes two sets of convolutional layers, each followed by a subsampling (pooling) layer.\n",
        "     - The first convolutional layer extracts low-level features, such as edges and simple textures.\n",
        "     - The second convolutional layer extracts higher-level features by combining information from the first layer.\n",
        "     - Subsampling layers perform down-sampling, reducing the spatial dimensions of the feature maps and providing translational invariance.\n",
        "\n",
        "   - **Fully Connected Layers:** After the convolutional and subsampling layers, LeNet-5 has three fully connected layers.\n",
        "     - These layers flatten the feature maps and connect all neurons in one layer to all neurons in the next.\n",
        "     - The last fully connected layer produces the final classification output.\n",
        "\n",
        "   - **Activation Functions:** LeNet-5 typically uses the sigmoid or hyperbolic tangent (tanh) activation functions in the hidden layers. A softmax activation function is used in the output layer for multi-class classification.\n",
        "\n",
        "   - **Pooling:** LeNet-5 uses average pooling in the subsampling layers to reduce the spatial dimensions and capture essential information.\n",
        "\n",
        "   - **Weight Sharing:** A key innovation in LeNet-5 is weight sharing, where the same set of weights is used for different regions of the input image. This reduces the number of parameters in the network and helps with generalization.\n",
        "\n",
        "# ans3.\n",
        " **Advantages and Limitations of LeNet-5:**\n",
        "\n",
        "   **Advantages:**\n",
        "   - **Pioneering Architecture:** LeNet-5 was one of the first successful CNN architectures, setting the foundation for modern CNNs.\n",
        "   - **Effective Feature Extraction:** It demonstrated the effectiveness of convolutional layers in automatically learning hierarchical features from images, which is crucial for image classification tasks.\n",
        "   - **Efficient Training:** Due to its relatively small size compared to modern architectures, LeNet-5 can be trained with limited computational resources.\n",
        "\n",
        "   **Limitations:**\n",
        "   - **Limited Complexity:** LeNet-5 is a relatively shallow network compared to modern deep learning architectures like ResNet and Inception. It may struggle with more complex image recognition tasks.\n",
        "   - **Fixed Input Size:** It works with fixed-size input images, which can be a limitation for applications requiring variable-sized inputs.\n",
        "   - **Sigmoid Activation:** The use of sigmoid or tanh activations can lead to vanishing gradient problems in deeper networks. Modern networks often use ReLU (Rectified Linear Unit) activations.\n",
        "   - **Noisy Inputs:** LeNet-5 may not perform well with noisy or distorted input images, as it was primarily designed for clean handwritten digits."
      ],
      "metadata": {
        "id": "gj2YqVYZZjVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ans 4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights.**"
      ],
      "metadata": {
        "id": "o7TisnglZvn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhNNOxRUZr-n",
        "outputId": "23977e67-4219-4920-fdc2-01e1ab40d232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define data transformations and create data loaders\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = LeNet5().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluation\n",
        "net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIVVCVk2Z0OU",
        "outputId": "86a252f3-7f11-45d3-e1dc-f9800372ec57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 110837528.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 116476628.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28412286.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4723661.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch 1, Loss: 0.24751385605073908\n",
            "Epoch 2, Loss: 0.06835265182850282\n",
            "Epoch 3, Loss: 0.048409525335315604\n",
            "Epoch 4, Loss: 0.03986614032407482\n",
            "Epoch 5, Loss: 0.03449499773784731\n",
            "Epoch 6, Loss: 0.02941565883594991\n",
            "Epoch 7, Loss: 0.024827035784752153\n",
            "Epoch 8, Loss: 0.021460649465632215\n",
            "Epoch 9, Loss: 0.018832606170457563\n",
            "Epoch 10, Loss: 0.01678426080469586\n",
            "Test Accuracy: 99.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing AlexNet"
      ],
      "metadata": {
        "id": "vu5RyJtXZ5FQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ans 1.\n",
        "**Overview of AlexNet:**\n",
        "   AlexNet is a deep convolutional neural network (CNN) architecture that gained significant attention and marked a breakthrough in computer vision when it won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet demonstrated the power of deep learning in image classification tasks.\n",
        "\n",
        "# ans 2.\n",
        "**Architectural Innovations in AlexNet:**\n",
        "   AlexNet introduced several key architectural innovations that contributed to its breakthrough performance:\n",
        "\n",
        "   - **Deep Architecture:** AlexNet was one of the first CNNs to have a relatively deep architecture compared to previous models. It consisted of eight layers, including five convolutional layers followed by three fully connected layers.\n",
        "\n",
        "   - **Rectified Linear Unit (ReLU) Activations:** Instead of traditional activation functions like sigmoid or tanh, AlexNet used ReLU activations in its hidden layers. ReLU helps mitigate the vanishing gradient problem, enabling faster training and better convergence.\n",
        "\n",
        "   - **Local Response Normalization (LRN):** AlexNet employed LRN after the ReLU activations in the first few layers. LRN enhanced the network's ability to generalize by normalizing neuron activations and promoting competition among adjacent neurons.\n",
        "\n",
        "   - **Overlapping Pooling:** Unlike traditional non-overlapping pooling layers, AlexNet used overlapping max-pooling layers with a stride smaller than the pool size. This allowed for better translation invariance and spatial hierarchies.\n",
        "\n",
        "   - **Dropout Regularization:** Dropout was applied to the fully connected layers during training, which helped reduce overfitting by randomly deactivating a fraction of neurons during each forward and backward pass.\n",
        "\n",
        "   - **Data Augmentation:** AlexNet used extensive data augmentation techniques, including random cropping and flipping of training images, to increase the diversity of the training data and improve generalization.\n",
        "\n",
        "   - **Parallel Processing:** AlexNet was designed to take advantage of the computational power of GPUs, allowing for efficient parallel processing of convolutions and reducing training time.\n",
        "\n",
        "# ans 3.\n",
        "**Role of Layers in AlexNet:**\n",
        "   - **Convolutional Layers:** The convolutional layers in AlexNet are responsible for feature extraction. The first convolutional layer detects simple features like edges, while subsequent layers capture increasingly complex and abstract features. These layers are followed by ReLU activations and LRN in the early layers.\n",
        "\n",
        "   - **Pooling Layers:** Max-pooling layers in AlexNet downsample the feature maps, reducing their spatial dimensions and providing translation invariance. The use of overlapping pooling helps capture more information.\n",
        "\n",
        "   - **Fully Connected Layers:** The fully connected layers at the end of AlexNet combine the features learned from convolutional and pooling layers and perform the final classification. Dropout is applied to these layers for regularization."
      ],
      "metadata": {
        "id": "kVbuH3kyaI0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ans 4\n",
        "Implement AlexNet using a deep learning"
      ],
      "metadata": {
        "id": "UJSEGGsnX98j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd-r1xkwi-8z",
        "outputId": "6e6c8f8a-0476-4e53-a586-d7ec1f678754"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), 256 * 6 * 6)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YrkPR-NVjB0i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize AlexNet and optimizer\n",
        "net = AlexNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(10):\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         inputs, labels = data\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = net(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "#     print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}\")\n",
        "\n",
        "# # Evaluate AlexNet on the test set\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for data in testloader:\n",
        "#         inputs, labels = data\n",
        "#         outputs = net(inputs)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f\"Accuracy on the test set: {100 * correct / total}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sG3EUwKjGtg",
        "outputId": "b3b38a4d-eda5-45f9-c0f1-119be6b9bdb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    }
  ]
}